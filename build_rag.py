from langchain_community.document_loaders import (
    WebBaseLoader,      # ç½‘é¡µ
    PyPDFLoader,        # PDF
    UnstructuredWordDocumentLoader,  # Word æ–‡æ¡£
    TextLoader,         # æ–‡æœ¬æ–‡ä»¶
    DirectoryLoader,    # ç›®å½•
    CSVLoader,          # CSV
)
import os
import re
from langchain_community.document_loaders import UnstructuredWordDocumentLoader, DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains.retrieval_qa.base import RetrievalQA
from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chains.combine_documents.refine import RefineDocumentsChain

def create_vectorstore(data_path: str, chunk_size: int = 1000):
    """
    data_path å¯ä»¥æ˜¯ï¼š
        1) å•ä¸ªæ–‡ä»¶è·¯å¾„: ./data/1.docx
        2) æ–‡ä»¶ç›®å½•è·¯å¾„: ./data/     ï¼ˆé‡Œé¢å¤šä¸ªword/pdf/txtï¼‰
    """

    persist_dir = "./chroma_db_all_docs"   # âœ… å¤šæ–‡æ¡£ç»Ÿä¸€å­˜å…¥åŒä¸€ä¸ªåº“

    # âœ… å¦‚æœå‘é‡åº“å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
    if os.path.exists(persist_dir):
        print(f"ğŸ”„ å‘ç°å·²å­˜åœ¨å‘é‡åº“ï¼Œç›´æ¥åŠ è½½: {persist_dir}")
        embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-large-zh-v1.5",
            model_kwargs={"device": "cuda"},
            encode_kwargs={"normalize_embeddings": True},
        )
        return Chroma(persist_directory=persist_dir, embedding_function=embeddings)

    print("ğŸ“¥ é¦–æ¬¡è¿è¡Œï¼Œå¼€å§‹æ„å»ºå‘é‡åº“...")

    # âœ… åˆ¤æ–­è·¯å¾„æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•
    docs = []
    if os.path.isdir(data_path):
        loader = DirectoryLoader(
            data_path,
            glob="**/*.pdf",  # å¯ä»¥æ”¹ä¸º "*.pdf" æˆ– "*.txt" æˆ–ç»„åˆ
            loader_cls=PyPDFLoader
        )
        docs = loader.load()
    else:
        loader = PyPDFLoader(data_path)
        docs = loader.load()
    #------------æ•°æ®æ¸…æ´—---------
    # å¯¹æ¯ä¸ªæ–‡æ¡£è¿›è¡Œæ–‡æœ¬æ¸…ç†
    for doc in docs:
        # åŒ¹é…ä»»ä½•å­—ç¬¦ä¹‹é—´çš„æ¢è¡Œç¬¦ï¼ˆåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ç­‰ï¼‰
        pattern = re.compile(r'(.)\n(.)', re.DOTALL)
        doc.page_content = re.sub(pattern, r'\1\2', doc.page_content)

    # pdf_page = docs[1]
    # print(f"æ¯ä¸€ä¸ªå…ƒç´ çš„ç±»å‹ï¼š{type(pdf_page)}.", 
    #     f"è¯¥æ–‡æ¡£çš„æè¿°æ€§æ•°æ®ï¼š{pdf_page.metadata}", 
    #     f"æŸ¥çœ‹è¯¥æ–‡æ¡£çš„å†…å®¹:\n{pdf_page.page_content}", 
    #     sep="\n------\n")
    
    print(f"ğŸ“„ æ–‡æ¡£è½½å…¥å®Œæˆï¼Œå…± {len(docs)} æ¡è®°å½•")

    # âœ… åˆ†å—
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=int(chunk_size * 0.1),
        separators=["\n\n", "\n", " ", ""]  # ä¼˜å…ˆæŒ‰æ®µè½åˆ†å‰²
    )
    splits = text_splitter.split_documents(docs)

    # âœ… åˆ›å»ºåµŒå…¥æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-large-zh-v1.5",
        model_kwargs={"device": "cuda"},
        encode_kwargs={"normalize_embeddings": True},
    )

    # âœ… å†™å…¥å‘é‡æ•°æ®åº“
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory=persist_dir
    )

    print("âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼å·²æŒä¹…åŒ–å­˜å‚¨ã€‚")
    return vectorstore

def evaluate_retrieval(retriever, test_cases):
    """è¯„ä¼°æ£€ç´¢å™¨æ€§èƒ½"""

    metrics = {
        "precision": [],  # ç²¾ç¡®ç‡
        "recall": [],     # å¬å›ç‡
    }

    for query, expected_doc_ids in test_cases:
        # æ‰§è¡Œæ£€ç´¢
        retrieved_docs = retriever.invoke(query)
        retrieved_ids = [doc.metadata['id'] for doc in retrieved_docs]

        # è®¡ç®—æŒ‡æ ‡
        relevant_retrieved = set(retrieved_ids) & set(expected_doc_ids)

        precision = len(relevant_retrieved) / \
            len(retrieved_ids) if retrieved_ids else 0
        recall = len(relevant_retrieved) / \
            len(expected_doc_ids) if expected_doc_ids else 0

        metrics["precision"].append(precision)
        metrics["recall"].append(recall)

    return {
        "avg_precision": sum(metrics["precision"]) / len(metrics["precision"]),
        "avg_recall": sum(metrics["recall"]) / len(metrics["recall"])
    }

def setup_qa_chain(vectorstore):
    """åˆ›å»ºåŸºäºæ£€ç´¢çš„é—®ç­”é“¾"""

    # åˆ›å»ºæ£€ç´¢å™¨
    retriever = vectorstore.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={
            "score_threshold": 0.1,  # åªè¿”å›ç›¸ä¼¼åº¦>0.2çš„ç»“æœ
            "k": 10
        }
    )

    # åˆ›å»º Ollama LLM å®ä¾‹
    ollama_llm = ChatOllama(
        model="qwen3:8b",
        temperature=0.1,
        max_tokens=1024,
        top_p=0.7,
        frequency_penalty=0,
        presence_penalty=0,
        stop=["<|endoftext|>"]
    )

    # # å®šä¹‰è‡ªå®šä¹‰promptæ¨¡æ¿ï¼ˆå¯é€‰ï¼‰
    # template = """ä½ æ˜¯ä¸€åä¿¡é“å»ºæ¨¡é¢†åŸŸçš„ä¸“ä¸šæŠ€æœ¯åŠ©æ‰‹ï¼Œè¯·ä½ åŸºäºä»¥ä¸‹æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œç®€æ˜æ‰¼è¦åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
    # è¦æ±‚ï¼š
    # 1. åªä½¿ç”¨æä¾›çš„æ–‡æ¡£å†…å®¹å›ç­”é—®é¢˜ï¼Œç¦æ­¢ä½¿ç”¨å¤–éƒ¨çŸ¥è¯†ã€‚
    # 2ã€‚ å¦‚æœæ–‡æ¡£å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œç›´æ¥è¯´æ˜â€œæ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç­”æ¡ˆâ€ã€‚
    # 3. å¼•ç”¨å…·ä½“çš„æ–‡æ¡£ç‰‡æ®µä½œä¸ºä¾æ®ï¼Œç¡®ä¿å›ç­”å‡†ç¡®ä¸”æœ‰æ®å¯ä¾ã€‚
    # 4. å›ç­”éœ€ä¸“ä¸šã€ä¸¥è°¨ã€é€»è¾‘æ¸…æ™°ã€è¿è´¯è‡ªç„¶ã€‚

    # æ–‡æ¡£å†…å®¹ï¼š
    # {context}

    # ç”¨æˆ·é—®é¢˜ï¼š{question}

    # è¯·ç»™å‡ºç®€æ˜æ‰¼è¦çš„å›ç­”ï¼š
    # """

    # prompt = PromptTemplate(
    #     template=template,
    #     input_variables=["context", "question"]
    # )


    # # åˆ›å»ºé—®ç­”é“¾
    # qa_chain = RetrievalQA.from_chain_type(
    #     llm=ollama_llm,
    #     chain_type="refine",    # ä½¿ç”¨ refine ç­–ç•¥
    #     retriever=retriever,
    #     chain_type_kwargs={"prompt": prompt},
    #     return_source_documents=True
    # )

    # å®šä¹‰åˆå§‹ promptï¼ˆç”¨äºç¬¬ä¸€ä¸ªæ–‡æ¡£ï¼‰
    initial_template = """ä½ æ˜¯ä¸€åä¿¡é“å»ºæ¨¡é¢†åŸŸçš„ä¸“ä¸šæŠ€æœ¯åŠ©æ‰‹ï¼Œä¸‹é¢æ˜¯æ–‡æ¡£ç‰‡æ®µå’Œç”¨æˆ·é—®é¢˜ï¼Œè¯·æ ¹æ®æ–‡æ¡£å†…å®¹ç»™å‡ºåˆæ­¥å›ç­”ã€‚

    æ–‡æ¡£å†…å®¹ï¼š
    {context}

    ç”¨æˆ·é—®é¢˜ï¼š{question}

    è¯·ç»™å‡ºç®€æ˜æ‰¼è¦çš„åˆæ­¥å›ç­”ï¼š"""

    initial_prompt = PromptTemplate(
        template=initial_template,
        input_variables=["context", "question"]
    )

    # å®šä¹‰ç²¾ç‚¼ promptï¼ˆç”¨äºåç»­æ–‡æ¡£è¿­ä»£ç²¾ç‚¼ï¼‰
    refine_template = """ä½ æ˜¯ä¸€åä¿¡é“å»ºæ¨¡é¢†åŸŸçš„ä¸“ä¸šæŠ€æœ¯åŠ©æ‰‹ï¼Œæˆ‘ä»¬å°†æ ¹æ®æ–°çš„æ–‡æ¡£å†…å®¹ï¼Œåœ¨ä¿æŒåŸå›ç­”é€»è¾‘ä¸¥è°¨çš„å‰æä¸‹ï¼Œæ”¹è¿›ä¹‹å‰ç»™å‡ºçš„å›ç­”ã€‚

    å·²æœ‰å›ç­”ï¼š{prev_response}

    ç°åœ¨æœ‰æ–°çš„æ–‡æ¡£å†…å®¹éœ€è¦å‚è€ƒï¼š
    {context}

    è¯·åŸºäºæ–°æ–‡æ¡£å†…å®¹å®Œå–„å·²æœ‰å›ç­”ï¼š
    - å¦‚æœæ–°å†…å®¹ä¸å·²æœ‰å›ç­”ä¸€è‡´ï¼Œä¿æŒåŸå›ç­”
    - å¦‚æœæ–°å†…å®¹æä¾›äº†æ›´å¤šä¿¡æ¯ï¼Œè¡¥å……åˆ°å›ç­”ä¸­
    - å¦‚æœæ–°å†…å®¹ä¸å·²æœ‰å›ç­”çŸ›ç›¾ï¼Œä»¥æ–°å†…å®¹ä¸ºå‡†

    å®Œå–„åçš„å›ç­”ï¼š"""

    refine_prompt = PromptTemplate(
        template=refine_template,
        input_variables=["prev_response", "context", "question"]
    )

    # ---------------------
    # 2) å°è£… LLMChain
    # ---------------------
    initial_llm_chain = LLMChain(llm=ollama_llm, prompt=initial_prompt)
    refine_llm_chain = LLMChain(llm=ollama_llm, prompt=refine_prompt)

    # ---------------------
    # 3) æ–‡æ¡£æ ¼å¼åŒ–æ¨¡æ¿
    # ---------------------
    document_prompt = PromptTemplate(
        input_variables=["page_content"],
        template="{page_content}"
    )
    # -----------------
    # 4) åˆ›å»º RefineDocumentsChain
    # -----------------
    refine_chain = RefineDocumentsChain(
        initial_llm_chain=initial_llm_chain,
        refine_llm_chain=refine_llm_chain,
        document_prompt=document_prompt,
        document_variable_name="context",     # ä¸ prompt ä¸­çš„å˜é‡å¯¹åº”
        initial_response_name="prev_response"  # ä¸ refine prompt ä¸­çš„å˜é‡å¯¹åº”
    )
    # -----------------
    # 5) åˆ›å»º RetrievalQA
    # -----------------
    qa_chain = RetrievalQA(
        retriever=retriever,
        combine_documents_chain=refine_chain,
        return_source_documents=True,
    )
    # åˆ›å»ºé—®ç­”é“¾
    # qa_chain = RetrievalQA.from_chain_type(
    #     #llm=ollama_llm,
    #     chain_type="refine",
    #     retriever=retriever,
    #     chain_type_kwargs={
    #         "initial_llm_chain": initial_llm_chain,
    #         "refine_llm_chain": refine_llm_chain,
    #     },
    #     return_source_documents=True
    # )

    print("âœ… refine æ¨¡å¼ QA é“¾æ„å»ºå®Œæˆï¼")

    return qa_chain

def query(question: str, qa_chain, show_sources: bool = True): 
    """æ‰§è¡Œé—®ç­”ï¼Œå¢åŠ å¼‚å¸¸æ•è·"""

    try:
        # è°ƒç”¨ QA é“¾
        result = qa_chain.invoke({"query": question})
        answer = result.get('result', "æ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç­”æ¡ˆ")

        # æ˜¾ç¤ºå‚è€ƒæ¥æº
        if show_sources and 'source_documents' in result:
            print("\nğŸ“š å‚è€ƒæ¥æº:")
            for doc in result['source_documents']:
                print(f"- æ¥æº: {doc.metadata.get('source', 'N/A')}")
                print(f"  å†…å®¹: {doc.page_content[:200]}...\n")

    except (IndexError, KeyError, TypeError) as e:
        # æ•è· refine_chain å†…éƒ¨ docs ä¸ºç©ºç­‰å¼‚å¸¸
        answer = "æ–‡æ¡£ä¸­æœªæ‰¾åˆ°ç­”æ¡ˆ"
        if show_sources:
            print("\nğŸ“š å‚è€ƒæ¥æº: æ— ")

    return answer

if __name__ == "__main__":
    vectorstore = create_vectorstore("./data")

    qa_chain = setup_qa_chain(vectorstore)

    answer = query("ä»€ä¹ˆæ˜¯äººä½“é˜´å½±ï¼Ÿ", qa_chain, show_sources=True)
    print("\nğŸ¤– å›ç­”:")
    print(answer)